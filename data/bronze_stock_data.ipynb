{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59fa0d81-a9cf-41f0-b1c7-8dab9f46ac67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b9b62b1-5818-486a-bdee-e1b180818055",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Формування дати для зчитування файлу\n",
    "date_previous = (datetime.today()- timedelta(days=1)).strftime('%Y_%m_%d')\n",
    "file_path = f\"/FileStore/tables/stock_prices_{date_previous}.csv\"  # /dbfs для Python-level доступу\n",
    "\n",
    "# Визначаємо: чи це історичний (заголовок) чи щоденний файл (без заголовка)\n",
    "first_file = \"2025_06_10\" in file_path  \n",
    "\n",
    "# Читаємо з відповідним header-параметром\n",
    "bronze_df = (\n",
    "    spark.read\n",
    "    .option(\"header\", first_file)  # True — якщо файл з заголовком, False — якщо без\n",
    "    .option(\"inferSchema\", False)  # Схема не визначається автоматично\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .csv(file_path)\n",
    ")\n",
    "\n",
    "# Якщо файл без заголовка, явно задаємо назви колонок\n",
    "if not first_file:\n",
    "    column_names = [\"Date\", \"Company\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]\n",
    "    bronze_df = bronze_df.toDF(*column_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3bdb753-093c-40d2-8568-a67088b2544e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Запис сирих(raw) даних у delta таблицю \"bronze_stock\"\n",
    "bronze_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/FileStore/delta/bronze_stock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df7b14b9-ece6-44a8-96fb-97b134b0c8cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Визначення схеми для таблиці логування\n",
    "\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), False),\n",
    "    StructField(\"pipeline_level\", StringType(), False),\n",
    "    StructField(\"source_file\", StringType(), True),\n",
    "    StructField(\"record_count\", IntegerType(), True),\n",
    "    StructField(\"null_count\", IntegerType(), True),\n",
    "    StructField(\"duplicates_removed\", IntegerType(), True),\n",
    "    StructField(\"gold_daily_count\", IntegerType(), True),\n",
    "    StructField(\"gold_weekly_count\", IntegerType(), True),\n",
    "    StructField(\"gold_monthly_count\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d196293-bdd7-48ce-b736-43a43b54f6d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Додавання даних до лог-таблиці\n",
    "log_data = [(\n",
    "    datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"bronze_stock_data\",\n",
    "    file_path,\n",
    "    bronze_df.count(),\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    None\n",
    ")]\n",
    "\n",
    "log_bronze_df = spark.createDataFrame(log_data, schema=log_schema)\n",
    "#Запис даних до лог-таблиці \"etl_log\" у форматі delta\n",
    "log_bronze_df.write.format(\"delta\").mode(\"append\").save(\"/FileStore/delta/etl_log\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1341733997752781,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_stock_data",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}